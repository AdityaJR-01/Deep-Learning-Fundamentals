{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23e688ea-8c9d-4b7f-963f-a39f0b4c31b7",
   "metadata": {},
   "source": [
    "# Name: Aditya Jayaraman"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fabd28-cdb0-45df-94df-13d5f1296ead",
   "metadata": {},
   "source": [
    "### Section: AI&ML\n",
    "### Roll no: 22WU0104004"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de15fe8-6af6-4193-87a6-2cd7220c6880",
   "metadata": {},
   "source": [
    "## Experiment-1 (Simple Neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "577831d9-b494-4033-9be1-dfa4b057a821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of the artificial neuron: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# step function Activation function\n",
    "def step_function(x):\n",
    "    return 1 if x >= 0 else 0\n",
    "\n",
    "def artificial_neuron(inputs, weights, bias):\n",
    "    # Calculate the weighted sum of inputs + bias\n",
    "    weighted_sum = np.dot(inputs, weights) + bias\n",
    "    # Apply the activation function\n",
    "    output = step_function(weighted_sum)\n",
    "    return output\n",
    "\n",
    "\n",
    "inputs = np.array([0, 1])  # Example binary input: [0, 1]\n",
    "weights = np.array([0.5, -0.5])  # Random weights for the inputs\n",
    "\n",
    "bias = 0.5 \n",
    "\n",
    "# Perform the classification\n",
    "output = artificial_neuron(inputs, weights, bias)\n",
    "\n",
    "print(\"Output of the artificial neuron:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8011c92e-126e-458f-85b0-a096ccb7abe7",
   "metadata": {},
   "source": [
    "## Experiment-2 (AND GATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b00cece6-c920-4b1d-a3d7-1617d88a9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 0. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def activation(self, z):\n",
    "        return np.heaviside(z, 0)  # Heaviside step function (activation)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Initializing weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Iterating through the entire training set for the given number of epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                z = np.dot(X[i], self.weights) + self.bias  # Finding dot product for each sample\n",
    "                y_pred = self.activation(z)  # Passing through the activation function\n",
    "\n",
    "                # Updating weights and bias based on the error\n",
    "                error = y[i] - y_pred\n",
    "                self.weights += self.learning_rate * error * X[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)  # Return predictions using the activation function\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # AND Gate training data\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "    y = np.array([0, 0, 0, 1])  # Output labels for AND gate\n",
    "\n",
    "    # Initialize and train the perceptron\n",
    "    perceptron = Perceptron(learning_rate=0.1, epochs=1000)\n",
    "    perceptron.fit(X, y)\n",
    "\n",
    "    # Test the perceptron\n",
    "    predictions = perceptron.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de0a77f-9a1f-4f27-8e2d-953d820ed295",
   "metadata": {},
   "source": [
    "## Experiment-2 (OR GATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69c5efa2-24b4-4399-a8b3-5a1f932da7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def activation(self, z):\n",
    "        return np.heaviside(z, 0)  # Heaviside step function (activation)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Initializing weights and bias\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        # Iterating through the entire training set for the given number of epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            for i in range(len(X)):\n",
    "                z = np.dot(X[i], self.weights) + self.bias  # Finding dot product for each sample\n",
    "                y_pred = self.activation(z)  # Passing through the activation function\n",
    "\n",
    "                # Updating weights and bias based on the error\n",
    "                error = y[i] - y_pred\n",
    "                self.weights += self.learning_rate * error * X[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "        return self.weights, self.bias\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.weights) + self.bias\n",
    "        return self.activation(z)  # Return predictions using the activation function\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # OR Gate training data\n",
    "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "    y = np.array([0, 1, 1, 1])  # Output labels for OR gate\n",
    "\n",
    "    # Initialize and train the perceptron\n",
    "    perceptron = Perceptron(learning_rate=0.1, epochs=1000)\n",
    "    perceptron.fit(X, y)\n",
    "\n",
    "    # Test the perceptron\n",
    "    predictions = perceptron.predict(X)\n",
    "    print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf209a5-f443-4d7b-b7e9-d806132f24df",
   "metadata": {},
   "source": [
    "## #Experiment-2 (Single Layer Perceptron Classification based on available datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0fed1194-6118-4a15-89bc-eef4cb5669e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (0, 1)] # sepal length, sepal width\n",
    "y = (iris.target == 0).astype(int)   #this line of code creates a binary target variable where the value is 1\n",
    "                                        #if the corresponding element in iris.target is 0 (indicating a specific class in the Iris dataset) and 0 otherwise.\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "class Perceptron:\n",
    "\n",
    "    def __init__(self, learning_rate, epochs):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    # heaviside activation function\n",
    "    def activation(self, z):\n",
    "        return np.heaviside(z, 0) \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Initializing weights and bias\n",
    "        self.weights = np.zeros((n_features))\n",
    "        self.bias = 0\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "\n",
    "            # Traversing through the entire training set\n",
    "            for i in range(len(X)):\n",
    "                z = np.dot(X, self.weights) + self.bias # Finding the dot product and adding the bias\n",
    "                y_pred = self.activation(z) # Passing through an activation function\n",
    "\n",
    "                #Updating weights and bias\n",
    "                self.weights = self.weights + self.learning_rate * (y[i] - y_pred[i]) * X[i]\n",
    "                self.bias = self.bias + self.learning_rate * (y[i] - y_pred[i])\n",
    "\n",
    "        return self.weights, self.bias\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "            z = np.dot(X, self.weights) + self.bias\n",
    "            return self.activation(z)\n",
    "\n",
    "perceptron = Perceptron(0.001, 100)\n",
    "\n",
    "perceptron.fit(X_train, y_train)\n",
    "\n",
    "pred = perceptron.predict(X_test)\n",
    "accuracy_score(pred, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976fdd70-5502-4dfe-8827-8f97c07d49ea",
   "metadata": {},
   "source": [
    "## Experiment-3 (Multi-Layer Perceptron for XOR gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54720298-4e18-4c37-98e3-0a30a2e7f9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 75.00%\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
      "Predictions: [0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Inputs\n",
    "y = np.array([[0], [1], [1], [0]])              # Outputs\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(2, input_dim=2, activation='relu'),  # Hidden layer with 2 neurons\n",
    "    Dense(1, activation='sigmoid')             # Output layer\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.01), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Evaluation\n",
    "loss, accuracy = model.evaluate(X, y, verbose=0)\n",
    "print(f\"Training Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Prediction\n",
    "predictions = model.predict(X)\n",
    "predictions = (predictions > 0.5).astype(int)  # Threshold at 0.5\n",
    "print(\"Predictions:\", predictions.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61459acc-06b9-47bd-a108-8b5e99498949",
   "metadata": {},
   "source": [
    "## Experiment-4 (Implementing Activation Functions on the Titanic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb6613f8-68be-4e74-84ab-3720c807a858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aditya\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.5298 - loss: 0.7156 - val_accuracy: 0.6503 - val_loss: 0.6221\n",
      "Epoch 2/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7055 - loss: 0.5621 - val_accuracy: 0.7692 - val_loss: 0.5483\n",
      "Epoch 3/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8087 - loss: 0.4877 - val_accuracy: 0.7762 - val_loss: 0.5119\n",
      "Epoch 4/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8075 - loss: 0.4513 - val_accuracy: 0.7902 - val_loss: 0.4959\n",
      "Epoch 5/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8273 - loss: 0.3927 - val_accuracy: 0.7902 - val_loss: 0.4856\n",
      "Epoch 6/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8343 - loss: 0.4101 - val_accuracy: 0.7972 - val_loss: 0.4755\n",
      "Epoch 7/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8251 - loss: 0.4081 - val_accuracy: 0.8042 - val_loss: 0.4759\n",
      "Epoch 8/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8170 - loss: 0.4044 - val_accuracy: 0.8042 - val_loss: 0.4726\n",
      "Epoch 9/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8197 - loss: 0.4201 - val_accuracy: 0.8112 - val_loss: 0.4733\n",
      "Epoch 10/10\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8253 - loss: 0.4018 - val_accuracy: 0.8042 - val_loss: 0.4681\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8206 - loss: 0.4405\n",
      "Accuracy: 80.42%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# Load the dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "titanic = pd.read_csv(url)\n",
    "print(titanic.head())\n",
    "\n",
    "# Preprocess the data\n",
    "titanic = titanic.dropna(subset=['Age', 'Embarked'])  # Drop rows with missing Age or Embarked\n",
    "titanic['Sex'] = titanic['Sex'].map({'male': 0, 'female': 1})  # Convert Sex to numerical\n",
    "\n",
    "# Encode 'Embarked' and 'Pclass' as categorical variables (one-hot encoding)\n",
    "titanic = pd.get_dummies(titanic, columns=['Embarked', 'Pclass'], drop_first=True)\n",
    "\n",
    "# Select features and label\n",
    "X = titanic[['Pclass_2', 'Pclass_3', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked_Q', 'Embarked_S']]\n",
    "y = titanic['Survived']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Build a neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden layer with ReLU activation\n",
    "model.add(Dense(64, input_dim=X_train.shape[1]))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Another hidden layer with Tanh activation\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('tanh'))\n",
    "\n",
    "# Output layer with Sigmoid activation\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6128a0-fa70-48bf-a727-06ac6e65d0e2",
   "metadata": {},
   "source": [
    "## Experiment-5 (Manual Implementation of forward and backpropagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b65badea-1286-430b-b76b-547c5c112904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following are the weights: \n",
      "Weight of w13:  0.1\n",
      "Weight of w23:  0.8\n",
      "Weight of w14:  0.4\n",
      "Weight of w24:  0.6\n",
      "Weight of w35:  0.3\n",
      "Weight of w45:  0.9\n",
      "Value of H3:  0.755\n",
      "Value of H4:  0.68\n",
      "The output is:  0.801444986673512\n",
      "__________________________________\n",
      "\n",
      "Utilizing Activation Function Formula...\n",
      "value for H3: 0.6802671966986485\n",
      "value for H4: 0.6637386974043528\n",
      "\n",
      "Output Value 0.6902834929076443\n",
      "__________________________________\n",
      "\n",
      "Target Value:  0.5\n",
      "Obtained Value:  0.6902834929076443\n",
      "Error:  -0.19028349290764435\n",
      "__________________________________\n",
      "\n",
      "Backward Pass Error for H3:  -0.002654489030884742\n",
      "Backward Pass Error for H4:  -0.00817164506412987\n",
      "Backward Pass Error for Output:  -0.04068112511233903\n",
      "__________________________________\n",
      "\n",
      "Following are the updated weights after backpropagation: \n",
      "Weight of w13:  0.09990709288391904\n",
      "Weight of w23:  0.7997610959872204\n",
      "Weight of w14:  0.39971399242275546\n",
      "Weight of w24:  0.5992645519442283\n",
      "Weight of w35:  0.2972325965061282\n",
      "Weight of w45:  0.8972998363008993\n",
      "Value of H3:  0.755\n",
      "Value of H4:  0.679\n",
      "The output is:  0.7975699089133668\n",
      "__________________________________\n",
      "\n",
      "Utilizing Activation Function Formula...\n",
      "value for H3: 0.6802671966986485\n",
      "value for H4: 0.6635154712332201\n",
      "\n",
      "Output Value 0.6894544212482856\n",
      "__________________________________\n",
      "\n",
      "Target Value:  0.5\n",
      "Obtained Value:  0.6894544212482856\n",
      "Error after backpropagation:  -0.1894544212482856\n",
      "__________________________________\n",
      "\n",
      "Difference in errors post backpropagation:  -0.0008290716593587488\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def sig(x):\n",
    "    return 1 / (1 + np.exp(-x)) # Sigmoid Activation Function\n",
    "\n",
    "y_tar = 0.5 # Target Value\n",
    "\n",
    "\n",
    "x1 = 0.35 # Inputs\n",
    "x2 = 0.9\n",
    "\n",
    "w13 = 0.1 # Weights\n",
    "w14 = 0.4\n",
    "w23 = 0.8\n",
    "w24 = 0.6\n",
    "w35 = 0.3\n",
    "w45 = 0.9\n",
    "\n",
    "h3 = x1*w13 + x2*w23\n",
    "h3 = round(h3,3) \n",
    "\n",
    "h4 = x1*w14 + x2*w24 \n",
    "h4 = round(h4,3)\n",
    "\n",
    "o5 = sig(h3)* w35 + sig(h4) * w45 # Output Formula\n",
    "\n",
    "\n",
    "y_out = sig(o5)\n",
    "\n",
    "print(\"Following are the weights: \")\n",
    "print(\"Weight of w13: \", w13)\n",
    "print(\"Weight of w23: \", w23)\n",
    "print(\"Weight of w14: \", w14)\n",
    "print(\"Weight of w24: \", w24)\n",
    "print(\"Weight of w35: \", w35)\n",
    "print(\"Weight of w45: \", w45)\n",
    "print(\"Value of H3: \",h3)\n",
    "print(\"Value of H4: \",h4)\n",
    "print(\"The output is: \", o5)\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Utilizing Activation Function Formula...\")\n",
    "print(\"value for H3:\",sig(h3))\n",
    "print(\"value for H4:\",sig(h4))\n",
    "print()\n",
    "print(\"Output Value\",sig(o5))\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Target Value: \",y_tar)\n",
    "print(\"Obtained Value: \",y_out)\n",
    "Error = y_tar - y_out\n",
    "print(\"Error: \",Error)\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "B_Error1 = y_out*(1 - y_out)*(y_tar - y_out)\n",
    "\n",
    "B_Error2 = sig(h3)*(1 - sig(h3))*(B_Error1*w35)\n",
    "\n",
    "B_Error3 = sig(h4)*(1 - sig(h4))*(B_Error1*w45)\n",
    "\n",
    "print(\"Backward Pass Error for H3: \",B_Error2)\n",
    "print(\"Backward Pass Error for H4: \",B_Error3)\n",
    "print(\"Backward Pass Error for Output: \",B_Error1)\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "\n",
    "learning_rate = 0.1\n",
    "w35 += learning_rate * B_Error1 * sig(h3)\n",
    "w45 += learning_rate * B_Error1 * sig(h4)\n",
    "w13 += learning_rate * B_Error2 * x1\n",
    "w14 += learning_rate * B_Error3 * x1\n",
    "w23 += learning_rate * B_Error2 * x2\n",
    "w24 += learning_rate * B_Error3 * x2\n",
    "\n",
    "\n",
    "h3 = x1*w13 + x2*w23\n",
    "h3 = round(h3,3)\n",
    "\n",
    "h4 = x1*w14 + x2*w24 \n",
    "h4 = round(h4,3)\n",
    "\n",
    "o5 = sig(h3)* w35 + sig(h4) * w45\n",
    "\n",
    "y_out = sig(o5)\n",
    "\n",
    "\n",
    "print(\"Following are the updated weights after backpropagation: \")\n",
    "print(\"Weight of w13: \", w13)\n",
    "print(\"Weight of w23: \", w23)\n",
    "print(\"Weight of w14: \", w14)\n",
    "print(\"Weight of w24: \", w24)\n",
    "print(\"Weight of w35: \", w35)\n",
    "print(\"Weight of w45: \", w45)\n",
    "print(\"Value of H3: \",h3)\n",
    "print(\"Value of H4: \",h4)\n",
    "print(\"The output is: \", o5)\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "print(\"Utilizing Activation Function Formula...\")\n",
    "print(\"value for H3:\",sig(h3))\n",
    "print(\"value for H4:\",sig(h4))\n",
    "print()\n",
    "print(\"Output Value\",sig(o5))\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "print(\"Target Value: \",y_tar)\n",
    "print(\"Obtained Value: \",y_out)\n",
    "Error_after_backprop = y_tar - y_out\n",
    "print(\"Error after backpropagation: \", Error_after_backprop)\n",
    "print(\"__________________________________\")\n",
    "print()\n",
    "\n",
    "\n",
    "error_reduction = Error - Error_after_backprop\n",
    "print(\"Difference in errors post backpropagation: \", error_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6956c6c-6297-46bc-8398-5b0a13880bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
